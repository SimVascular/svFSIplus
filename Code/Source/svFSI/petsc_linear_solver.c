/* 
 * Author: Chi Zhu, Peking University
 *
 * All Rights Reserved.
 *
 * See Copyright-SimVascular.txt for additional details.
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject
 * to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
 * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
 * PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER
 * OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

#include "petsc_linear_solver.h"
#include <locale.h>

/*
    Nomenclature used in this file:
    - global: across all MPI processes
    - local: current MPI processes
    - natural id: global vertex id from mesh file
    - PETSc id: global vertex id used within PETSc
    - O1 ordering: the local order of natural id generated by ParMetis in svFSI
    - O2 ordering: reorder O1 to "(vertex shared with lower rank)  + 
                   (vertex owned exclusively by current rank) + 
                   (vertex shared with higher rank)"
    - owned vertices: "(vertex shared with lower rank)  + 
                      (vertex owned exclusively by current rank)" 
                      forms the vertices owned by the current rank.
    - ghost vertices: vertex shared with higher rank 

    The mapping between different ordering is
        O1 --> O2 --> PETSc ID
*/

/* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

     Functions that interact with Fortran

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */
/*
    Initialize PETSc and create lhs for PETSc.
*/
void petsc_initialize_(const PetscInt *nNo, const PetscInt *mynNo, \
                       const PetscInt *nnz, const PetscInt *nEq, \
                       const PetscInt *svFSI_ltg, const PetscInt *svFSI_map, \
                       const PetscInt *svFSI_rowPtr, const PetscInt *svFSI_colPtr, char *inp)
{   
    // char* in_file = rm_blank(inp);
    char* in_file = inp;
    if (access(in_file, F_OK) == 0) {
        PetscInitialize(NULL, NULL, in_file, NULL);
        PetscPrintf(MPI_COMM_WORLD, " <PETSC_INITIALIZE>: "
                "use linear solver config. from file.\n");
    }
    else {
        PetscInitialize(NULL, NULL, NULL, NULL);
        PetscPrintf(MPI_COMM_WORLD, " <PETSC_INITIALIZE>: "
                "use linear solver config. from svFSI input.\n");
    }
    
    PetscLogStageRegister("Create LHS", &stages[0]);
    PetscLogStageRegister("Create VecMat", &stages[1]);
    PetscLogStageRegister("Create Solver", &stages[2]);
    PetscLogStageRegister("Set Values", &stages[3]);
    PetscLogStageRegister("PETSc Solve", &stages[4]);
    PetscLogStageRegister("Row-Col. Sca.", &stages[5]);

    PetscLogStagePush(stages[0]);

    plhs.created = PETSC_FALSE;
    petsc_create_lhs(*nNo, *mynNo, *nnz, svFSI_ltg, svFSI_map, svFSI_rowPtr, svFSI_colPtr);

    PetscMalloc1(*nEq, &psol);
    for (PetscInt i = 0; i < *nEq; i++) psol[i].created = PETSC_FALSE;

    PetscLogStagePop();

}

/*
    Create parallel vector and matrix data structures.
*/
void petsc_create_linearsystem_(const PetscInt *dof, const PetscInt *iEq, const PetscInt *nEq, \
                                const PetscReal *svFSI_DirBC, const PetscReal *svFSI_lpBC)
{   
    // PetscInt cEq = *iEq - 1;
    PetscInt cEq = *iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;

    if (psol[cEq].created) return;

    PetscLogStagePush(stages[1]);
    petsc_create_bc(*dof, cEq, svFSI_DirBC, svFSI_lpBC); /* bc info is required for mat_create */
    petsc_create_vecmat(*dof, cEq, *nEq);
    psol[cEq].created = PETSC_TRUE;
    PetscLogStagePop();
}

/*
    Create PETSc linear solver data.
*/
void petsc_create_linearsolver_(const PetscInt *lsType, const PetscInt *pcType, \
                                const PetscInt *kSpace, const PetscInt *maxIter, \
                                const PetscReal *relTol, const PetscReal *absTol, \
                                const PetscInt *phys, const PetscInt *dof, \
                                const PetscInt *iEq, const PetscInt *nEq)
{   
    // PetscInt cEq = *iEq - 1;
    PetscInt cEq = *iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;
    PC pc;
    PetscBool usefieldsplit;

    PetscLogStagePush(stages[2]);

    /* Initialize equation specific prefix for vec/mat/ksp */
    switch (*phys)
    {
        case EQ_fluid:
            psol[cEq].pre = "ns_";
            break;
        case EQ_struct:
            psol[cEq].pre = "st_";
            break;
        case EQ_heatS:
            psol[cEq].pre = "hs_";
            break;
        case EQ_lElas:
            psol[cEq].pre = "le_";
            break;
        case EQ_heatF:
            psol[cEq].pre = "hf_";
            break;
        case EQ_FSI:
            psol[cEq].pre = "fs_";
            break;
        case EQ_mesh:
            psol[cEq].pre = "ms_";
            break;
        case EQ_shell:
            psol[cEq].pre = "sh_";
            break;
        case EQ_CMM:
            psol[cEq].pre = "cm_";
            break;
        case EQ_CEP:
            psol[cEq].pre = "ep_";
            break;
        case EQ_ustruct:
            psol[cEq].pre = "st_";
            break;
        case EQ_stokes:
            psol[cEq].pre = "ss_";
            break;
        default:
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_CREATE_LINEARSOLVER>: "
                "equation type %d is not defined.\n", *phys);
            break;
    }

    /* Initialize PETSc linear solver setting */
    KSPCreate(MPI_COMM_WORLD, &psol[cEq].ksp);
    if (*nEq > 1) KSPSetOptionsPrefix(psol[cEq].ksp, psol[cEq].pre);
    KSPSetTolerances(psol[cEq].ksp, *relTol, *absTol, PETSC_DEFAULT, *maxIter); 
    
    /* Set linear solver */
    switch (*lsType)
    {
        case PETSc_CG:
            KSPSetType(psol[cEq].ksp, KSPCG);
            break;
        case PETSc_GMRES:
            KSPSetType(psol[cEq].ksp, KSPGMRES);
//            KSPGMRESSetRestart(psol[cEq].ksp, *kSpace);
            break;
        case PETSc_BICGS:
            KSPSetType(psol[cEq].ksp, KSPBCGS);
            break;
        default:
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "linear solver type not supported through svFSI input file.\n"
            "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "More linear solver types can be set through petsc_option.inp.\n");
            break;
    }
    
    /* Set preconditioner */
    psol[cEq].rcs = PETSC_FALSE;
    KSPGetPC(psol[cEq].ksp, &pc);
    switch (*pcType)
    {
        case PETSc_PC:
        case PETSc_PC_FSILS:
            PCSetType(pc, PCJACOBI);
            break;
        case PETSc_PC_RCS:
            psol[cEq].rcs = PETSC_TRUE;
            PetscPrintf(MPI_COMM_WORLD, "WARNING <PETSC_CREATE_LINEARSOLVER>: "
            "precondition the linear system with RCS first.\n"
            "WARNING <PETSC_CREATE_LINEARSOLVER>: "
            "This will NOT be overwritten by petsc_option.inp!\n");
            break;
        default:
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "preconditioner type not supported through svFSI input file.\n"
            "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "More preconditioner options can be set through petsc_option.inp.\n");   
            break;
    }
    
    /* Run time options */
    KSPSetFromOptions(psol[cEq].ksp);

    /* Set up PCFIELDSPLIT */
    PetscObjectTypeCompare((PetscObject)pc, PCFIELDSPLIT, &usefieldsplit);
    if (usefieldsplit){
        if (*phys==EQ_fluid || *phys==EQ_ustruct || *phys==EQ_stokes ){
            petsc_set_pcfieldsplit(*dof, cEq);
        }
        else {
            PetscPrintf(MPI_COMM_WORLD, "///////////////////////////////////"
            "///////////////////////////////////////////////////////////////\n");
            PetscPrintf(MPI_COMM_WORLD, "WARNING <PETSC_CREATE_LINEARSOLVER>: "
            "PCFIELDSPLIT is hard-coded for ustruct/stokes/NS.\n");
            PetscPrintf(MPI_COMM_WORLD, "///////////////////////////////////"
            "///////////////////////////////////////////////////////////////\n");
        }
    }

    PetscLogStagePop();
}

/*
    Set up the linear system.
*/
void petsc_set_values_(const PetscInt *dof, const PetscInt *iEq, const PetscReal *R, \
                       const PetscReal *Val, const PetscReal *svFSI_DirBC, const PetscReal *svFSI_lpBC)
{   
    // PetscInt cEq = *iEq - 1;
    PetscInt cEq = *iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;

    /* Set values in A &b, apply Dir and Lumped parameter BC */
    PetscLogStagePush(stages[3]);
    petsc_set_vec(*dof, cEq, R);
    petsc_set_mat(*dof, cEq, Val);
    petsc_set_bc(cEq, svFSI_DirBC, svFSI_lpBC);
    PetscLogStagePop();

    /* Scale A and b if RCS preconditioner is activated. */
    PetscLogStagePush(stages[5]);
    if (psol[cEq].rcs){
        petsc_pc_rcs(*dof, cEq);
    }
    PetscLogStagePop();
}

/*
    Solve the linear system.
*/
void petsc_solve_(PetscReal *resNorm,  PetscReal *initNorm,  PetscReal *dB, \
                  PetscReal *execTime, bool *converged, PetscInt *numIter, \
                  PetscReal *R, const PetscInt *maxIter, const PetscInt *dof, \
                  const PetscInt *iEq)
{   
    PetscReal *a, *array;
    PetscInt   i, j, na;
    // PetscInt   cEq = *iEq - 1;
    PetscInt   cEq = *iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;
    PetscBool  usepreonly;
    KSPType    ksptype;
    Vec        lx, res;
    KSPConvergedReason reason;
    PetscLogDouble ts, te;

    PetscLogStagePush(stages[4]);
    na = *maxIter;
    PetscMalloc1(na, &a);
    PetscTime(&ts);
    KSPSetOperators(psol[cEq].ksp, psol[cEq].A, psol[cEq].A);

    /* Calculate residual for direct solver. KSP uses preconditioned norm. */
    PetscObjectTypeCompare((PetscObject)psol[cEq].ksp, KSPPREONLY, &usepreonly);
    if (usepreonly){
        VecNorm(psol[cEq].b, NORM_2 , initNorm);
    }
    else {
        KSPSetResidualHistory(psol[cEq].ksp, a, na, PETSC_TRUE);
    }
    KSPSetUp(psol[cEq].ksp);
    KSPSolve(psol[cEq].ksp, psol[cEq].b, psol[cEq].b);

    /* Rescale solution for RCS preconditioner */
    if (psol[cEq].rcs){
        VecPointwiseMult(psol[cEq].b, psol[cEq].b, psol[cEq].Dc);
    }

    /* Fill the ghost vertices with correct values. */
    VecGhostUpdateBegin(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD);
    VecGhostUpdateEnd(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD);

    PetscTime(&te);

    /* Get convergence info. */
    if (usepreonly){
        *resNorm   = __DBL_EPSILON__;
    }
    else {
        KSPGetResidualHistory(psol[cEq].ksp, (const PetscReal **) &a, &na);
        *initNorm  = a[0];
        *resNorm   = a[na-1];
    }
    KSPGetIterationNumber(psol[cEq].ksp, numIter);
    KSPGetConvergedReason(psol[cEq].ksp, &reason);
    *converged = reason > 0 ? true : false;
    *dB        = 10.0 * log(*resNorm / *initNorm);
    *execTime  = te - ts;

    /* Export solution to svFSI. */
    VecGhostGetLocalForm(psol[cEq].b, &lx);
    VecGetArray(lx, &array);
    na = 0;
    for (i = 0; i < plhs.nNo; i++) {
        for (j = 0; j < *dof; j++) {
            R[plhs.map[i]*(*dof)+j] = array[na++];
        }
    }
    VecRestoreArray(lx, &array);
    VecGhostRestoreLocalForm(psol[cEq].b, &lx);

    PetscFree(a);
    PetscLogStagePop();
}

/* 
    Clean up all petsc data. 
*/
void petsc_destroy_all_(const PetscInt *nEq)
{   
    if (!psol==NULL){
        PetscInt cEq, ierr;

        if (!plhs.created) {
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_DESTROY_ALL>: "
                    "lhs is not created.\n");
            ierr = PETSC_ERR_ARG_WRONGSTATE;
            PETSCABORT(MPI_COMM_WORLD, ierr);
        }

        plhs.nNo     = 0;
        plhs.mynNo   = 0;
        plhs.created = PETSC_FALSE;

        PetscFree (plhs.map);
        PetscFree2(plhs.rowPtr, plhs.colPtr);
        PetscFree2(plhs.ltg, plhs.ghostltg);

        for (cEq = 0; cEq < *nEq; cEq++)
        {   
            if (!psol[cEq].created) {
                PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_DESTROY_ALL>: "
                    "solver %d is not created.\n", cEq);
                ierr = PETSC_ERR_ARG_WRONGSTATE;
                PETSCABORT(MPI_COMM_WORLD, ierr);
            }

            psol[cEq].created = PETSC_FALSE;

            psol[cEq].lpPts = 0;
            PetscFree2(psol[cEq].lpBC_l, psol[cEq].lpBC_g);

            psol[cEq].DirPts = 0;
            PetscFree (psol[cEq].DirBC);

            VecDestroy(&psol[cEq].b);
            MatDestroy(&psol[cEq].A);
            KSPDestroy(&psol[cEq].ksp);

            if (psol[cEq].rcs) {
                psol[cEq].rcs = PETSC_FALSE;
                VecDestroy(&psol[cEq].Dr);
                VecDestroy(&psol[cEq].Dc);
            }
        }
        PetscFree(psol); 
        
        PetscFinalize();
    }
}


/* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

     Private functions

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */

/*
    Creating PETSc lhs data structure with svFSI info.
*/
PetscErrorCode petsc_create_lhs(const PetscInt nNo, const PetscInt mynNo, const PetscInt nnz, \
                                const PetscInt *svFSI_ltg, const PetscInt *svFSI_map, \
                                const PetscInt *svFSI_rowPtr, const PetscInt *svFSI_colPtr)
{   
    PetscInt  i, j, ierr;
    PetscInt *local2global, *local2local; /* local copy of svFSI ltg and map */
    PetscInt *local_ltg;                  /* local to global mapping of all vertices on current proc.*/
    PetscInt *owned_ltg;                  /* local to global mapping of owned vertices */
    PetscInt *ghost_ltg;                  /* local to global mapping of ghost vertices */
    PetscInt  rstart;                     /* starting index of PETSc ordering for a processor */
    AO        ao;                         /* Application Ordering object */
    PetscInt *pordering;                  /* PETSc ordering */
    PetscInt  ghostnNo;                   /* number of ghost vertices */

    PetscFunctionBeginUser;

    /* In cases with remeshing, lhs needs to be regenerated. */
    if (plhs.created) {
        PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_DESTROY_ALL>: \
                lhs is already created.");
        ierr = PETSC_ERR_ARG_WRONGSTATE;
        PETSCABORT(MPI_COMM_WORLD, ierr);
    }

    plhs.nNo     = nNo;
    plhs.mynNo   = mynNo;
    plhs.created = PETSC_TRUE;

    /* Fortran index to C index (NOT apply for svFSIplus) */ 
    PetscCall(PetscMalloc2(nNo, &local2global, nNo, &local2local));
    for (i = 0; i < nNo; i++) {
        // local2global[i] = svFSI_ltg[i] - 1;
        // local2local[i]  = svFSI_map[i] - 1;
        local2global[i] = svFSI_ltg[i];
        local2local[i]  = svFSI_map[i];
    }

    /* Create local mapping, map[O2] = O1 */
    PetscCall(PetscMalloc1(nNo, &plhs.map));
    for (i = 0; i < nNo; i++) {
        plhs.map[local2local[i]] = i;
    }

    /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
        Create global-to-global mapping using AO:
        - Global order 1 is from svFSI (ltg, i.e. O1).
        - Global order 2 is for PETSc (PETSc ordering). It is:
            |---- Proc 0-------|   |----------- Proc 2 ------------|
            0 1 ...   mynNo(0)-1    mynNo(0) ... mynNo(0)+mynNo(1)-1
        
        - Note that global order 2 locally follows O2 ordering, i.e.
           lower rank vtx + current rank vtx 
        - Vertices from higher rank are ghost vertices.
     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */
    /* Remap svFSI ltg to svFSI lhs%map order (O2) */
    PetscCall(PetscMalloc2(mynNo, &owned_ltg, nNo, &local_ltg));
    for (i = 0; i < nNo; i++) {
        j = local2local[i];
        local_ltg[j] = local2global[i];
    }
    for (i = 0; i < mynNo; i++) owned_ltg[i] = local_ltg[i];
    ghostnNo = nNo - mynNo;
    PetscCall(PetscMalloc1(ghostnNo, &ghost_ltg));
    for (i = 0; i < ghostnNo; i++) ghost_ltg[i] = local_ltg[i+mynNo];

    /* Create AO object between svFSI ltg and PETSc ordering */
    PetscCallMPI(MPI_Scan(&mynNo, &rstart, 1, MPIU_INT, MPI_SUM, MPI_COMM_WORLD));
    rstart -= mynNo;
    PetscCall(PetscMalloc1(mynNo, &pordering));
    for (i = 0; i < mynNo; i++) pordering[i] = rstart + i;
    PetscCall(AOCreateBasic(MPI_COMM_WORLD, mynNo, owned_ltg, pordering, &ao));

    /* 
        Now map the vertex id in natural ordering to PETSc ordering.
        Before AOApplicationToPetsc:
            local_ltg[i] = natural id, i = 0,..., nNo-1
        After AOApplicationToPetsc:
            local_ltg[i] = PETSc id, i = 0,..., nNo-1
    */
    PetscCall(AOApplicationToPetsc(ao, nNo, local_ltg));
    PetscCall(AOApplicationToPetsc(ao, ghostnNo, ghost_ltg));
    PetscCall(AODestroy(&ao));

    PetscCall(PetscMalloc2(nNo, &plhs.ltg, ghostnNo, &plhs.ghostltg));
    for (i = 0; i < nNo; i++) plhs.ltg[i] = local_ltg[i];
    for (i = 0; i < ghostnNo; i++) plhs.ghostltg[i] = ghost_ltg[i];

    /* Adjacency info in PETSc lhs (i.e plhs.rowPtr and plhs.colPtr) is used to set values. */
    PetscCall(PetscMalloc2(2*nNo, &plhs.rowPtr, nnz, &plhs.colPtr));
    for (i=0; i < nNo; i++) {
        // plhs.rowPtr[i*2]   = svFSI_rowPtr[i*2] - 1; 
        plhs.rowPtr[i*2]   = svFSI_rowPtr[i*2];
        // plhs.rowPtr[i*2+1] = svFSI_rowPtr[i*2+1];
        plhs.rowPtr[i*2+1] = svFSI_rowPtr[i*2+1]+1;
    }
    for (i=0; i < nnz; i++) {
        // plhs.colPtr[i] = plhs.ltg[svFSI_colPtr[i] - 1];
        plhs.colPtr[i] = plhs.ltg[svFSI_colPtr[i]];
    }
    
    /* Deallocate memory */
    PetscCall(PetscFree2(owned_ltg, local_ltg));
    PetscCall(PetscFree2(local2global, local2local));
    PetscCall(PetscFree(ghost_ltg));
    PetscCall(PetscFree(pordering));

    PetscFunctionReturn(0);
}

/*
    Creating PETSc data structure for Dirichlet and lumped parameter BC with svFSI info.
*/
PetscErrorCode petsc_create_bc(const PetscInt dof, const PetscInt cEq, \
                               const PetscReal *svFSI_DirBC, const PetscReal *svFSI_lpBC)
{   
    PetscInt  i, j, cc, ii;
    PetscInt *row1, *row2;
    PetscReal eps = 1.0e6*__DBL_EPSILON__;

    PetscFunctionBeginUser;

    /* Find PETSc global index of dofs with Dirichlet BC */
    PetscCall(PetscMalloc1(plhs.mynNo*dof, &row1));
    cc = 0;
    for (i = 0; i < plhs.mynNo; i++) {
        ii = i * dof;
        for (j = 0; j < dof; j++) {
            if ( ((int) svFSI_DirBC[ii+j]) == 0) {
                row1[cc++] = plhs.ltg[i]*dof + j;
            }
        }
    }
    PetscCall(PetscMalloc1(cc, &psol[cEq].DirBC));
    psol[cEq].DirPts = cc;
    for (i = 0; i < cc; i++){
        psol[cEq].DirBC[i] = row1[i];
    }

    /* Find O2 and PETSc index of dofs with lumped parameter BC */
    PetscCall(PetscMalloc1(plhs.mynNo*dof, &row2));
    cc = 0;
    for (i = 0; i < plhs.mynNo; i++) {
        ii = i * dof;
        for (j = 0; j < dof; j++) {
            if ( PetscAbsReal(svFSI_lpBC[ii+j]) > eps) {
                row1[cc] = ii + j;
                row2[cc] = plhs.ltg[i]*dof + j;
                cc++;
            }
        }
    }
    PetscCall(PetscMalloc2(cc, &psol[cEq].lpBC_l, cc, &psol[cEq].lpBC_g));
    psol[cEq].lpPts = cc;
    for (i = 0; i < cc; i++){
        psol[cEq].lpBC_l[i] = row1[i];
        psol[cEq].lpBC_g[i] = row2[i];
    }
    PetscCall(PetscFree(row1));
    PetscCall(PetscFree(row2));

    PetscFunctionReturn(0);
}

/*
    Create and preallocate parallel PETSC vector and matrix data structure.
*/
PetscErrorCode petsc_create_vecmat(const PetscInt dof, const PetscInt cEq, const PetscInt nEq)
{   
    PetscInt   i, j, is, ie, row, col;
    Mat        preallocator;
    PetscReal *value;
    PC         pc;
    PetscBool  usefieldsplit, useamg;

    PetscFunctionBeginUser;

    /* Create vector data structures */
    PetscCall(VecCreateGhostBlock(MPI_COMM_WORLD, dof, plhs.mynNo*dof, PETSC_DECIDE, plhs.nNo-plhs.mynNo, plhs.ghostltg, &psol[cEq].b));
    if (nEq > 1) PetscCall(VecSetOptionsPrefix(psol[cEq].b, psol[cEq].pre));
    PetscCall(VecSetFromOptions(psol[cEq].b));


    /*
        Preallocate psol.A with the help of MATPREALLOCATOR.
        Internally MatPreallocatorPreallocate() will call MatSetOption(A, MAT_NEW_NONZERO_ALLOCATION_ERR, PETSC_TRUE);
    */
    PetscCall(MatCreate(MPI_COMM_WORLD, &preallocator));
    PetscCall(MatSetType(preallocator, MATPREALLOCATOR));
    PetscCall(MatSetSizes(preallocator, plhs.mynNo*dof, plhs.mynNo*dof, PETSC_DECIDE, PETSC_DECIDE));
    PetscCall(MatSetBlockSize(preallocator, dof));
    PetscCall(MatSetUp(preallocator));
    PetscCall(PetscMalloc1(dof*dof, &value));
    for (i = 0; i < dof*dof; i++) value[i] = 0.0;
    /* Internal points */
    for (i = 0; i < plhs.nNo; i++) {
        is  = plhs.rowPtr[i*2];
        ie  = plhs.rowPtr[i*2+1];
        row = plhs.ltg[i];
        for (j = is; j < ie; j++) {
            col = plhs.colPtr[j];
            PetscCall(MatSetValuesBlocked(preallocator, 1, &row, 1, &col, value, INSERT_VALUES));
        }
    }
    /* Points with lumped parameter BC */
    for (i = 0; i < psol[cEq].lpPts; i++){
        row = psol[cEq].lpBC_g[i];
        for (j = 0; j < psol[cEq].lpPts; j++){
            col = psol[cEq].lpBC_g[j];
            PetscCall(MatSetValue(preallocator, row, col, 0.0, INSERT_VALUES));
        }
    }
    PetscCall(MatAssemblyBegin(preallocator, MAT_FINAL_ASSEMBLY));
    PetscCall(MatAssemblyEnd(preallocator, MAT_FINAL_ASSEMBLY));

    /* Create and preallocate matrix structure */
    KSPGetPC(psol[cEq].ksp, &pc);
    PetscObjectTypeCompare((PetscObject)pc, PCFIELDSPLIT, &usefieldsplit); /* Fieldsplit only supports MATAIJ */
    PetscObjectTypeCompare((PetscObject)pc, PCGAMG, &useamg); /* GAMG only supports MATAIJ */
    PetscCall(MatCreate(MPI_COMM_WORLD, &psol[cEq].A));
    if (dof > 1 && !usefieldsplit && !useamg) {
        PetscCall(MatSetType(psol[cEq].A, MATBAIJ));
    }
    else {
        PetscCall(MatSetType(psol[cEq].A, MATAIJ));
    }
    if (nEq > 1) PetscCall(MatSetOptionsPrefix(psol[cEq].A, psol[cEq].pre));
    PetscCall(MatSetFromOptions(psol[cEq].A));
    PetscCall(MatSetSizes(psol[cEq].A, plhs.mynNo*dof, plhs.mynNo*dof, PETSC_DECIDE, PETSC_DECIDE));
    PetscCall(MatSetBlockSize(psol[cEq].A, dof));
    PetscCall(MatPreallocatorPreallocate(preallocator, PETSC_TRUE, psol[cEq].A));
    PetscCall(MatSetOption(psol[cEq].A, MAT_NEW_NONZERO_LOCATION_ERR, PETSC_TRUE));
    PetscCall(MatSetOption(psol[cEq].A, MAT_NEW_NONZERO_LOCATIONS, PETSC_FALSE));

    /* Create vector structure for RCS preconditioner */
    if (psol[cEq].rcs){
        PetscCall(VecDuplicate(psol[cEq].b, &psol[cEq].Dr));
        PetscCall(VecDuplicate(psol[cEq].b, &psol[cEq].Dc));
    }

    PetscCall(PetscFree(value));
    PetscCall(MatDestroy(&preallocator));

    PetscFunctionReturn(0);
}

/*
    Set values to the rhs vector.
    R is the rhs from svFSI in O1 order.
*/
PetscErrorCode petsc_set_vec(const PetscInt dof, const PetscInt cEq, const PetscReal *R)
{
    PetscInt   indx, i;
    PetscInt  *row;
    const PetscReal *value;

    PetscFunctionBeginUser;

    /* Set the rhs vector using global index. */
    PetscCall(VecZeroEntries(psol[cEq].b));

    for (i = 0; i < plhs.mynNo; i++) {
        row   = plhs.ltg+i;
        indx  = plhs.map[i];
        value = R + indx*dof;
        PetscCall(VecSetValuesBlocked(psol[cEq].b, 1, row, value, INSERT_VALUES));
    }
    PetscCall(VecAssemblyBegin(psol[cEq].b));
    PetscCall(VecAssemblyEnd(psol[cEq].b));

    /* Fill the ghost vertices with correct values. */
    PetscCall(VecGhostUpdateBegin(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD));
    PetscCall(VecGhostUpdateEnd(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD));
    
    PetscFunctionReturn(0);
}

/*
    Set values to the matrix.
*/
PetscErrorCode petsc_set_mat(const PetscInt dof, const PetscInt cEq, const PetscReal *Val)
{   
    PetscInt   i, j, is, ie; 
    PetscInt  *col, *row;
    const PetscScalar *value;

    PetscFunctionBeginUser;

    PetscCall(MatZeroEntries(psol[cEq].A));
    for (i = 0; i < plhs.nNo; i++) {
        is    = plhs.rowPtr[i*2];
        ie    = plhs.rowPtr[i*2+1];
        row   = plhs.ltg+i;
        for (j = is; j < ie; j++){
            value = Val + j*dof*dof;
            col   = plhs.colPtr + j;
            PetscCall(MatSetValuesBlocked(psol[cEq].A, 1, row, 1, col, value, ADD_VALUES));  
        }
        
    }
    PetscCall(MatAssemblyBegin(psol[cEq].A, MAT_FLUSH_ASSEMBLY));
    PetscCall(MatAssemblyEnd(psol[cEq].A, MAT_FLUSH_ASSEMBLY));

    PetscFunctionReturn(0);
}

/*
    Set up Dirichlet BC and resistance BC.
*/
PetscErrorCode petsc_set_bc(const PetscInt cEq, const PetscReal *DirBC, const PetscReal *lpBC)
{   
    Vec x;
    PetscInt  i, j, ii, jj, row, col;
    PetscReal value;

    PetscFunctionBeginUser;

    /* Apply lumped parameter BC by augmenting the matrix A. */
    for (i = 0; i < psol[cEq].lpPts; i++){
        ii  = psol[cEq].lpBC_l[i];
        row = psol[cEq].lpBC_g[i];
        for (j = 0; j < psol[cEq].lpPts; j++){
            jj  = psol[cEq].lpBC_l[j];
            col = psol[cEq].lpBC_g[j];
            value = lpBC[ii] * lpBC[jj];
            PetscCall(MatSetValue(psol[cEq].A, row, col, value, ADD_VALUES));
        }
    }
    PetscCall(MatAssemblyBegin(psol[cEq].A, MAT_FINAL_ASSEMBLY));
    PetscCall(MatAssemblyEnd(psol[cEq].A, MAT_FINAL_ASSEMBLY));

    /*
        Apply Dirichlet BC by resetting matrix A and rhs b.
        Since the BC remains the same, the matrix will retain the same nonzero structure
    */
    PetscCall(VecDuplicate(psol[cEq].b, &x));
    PetscCall(VecPlaceArray(x, DirBC));
    PetscCall(MatZeroRowsColumns(psol[cEq].A, psol[cEq].DirPts, psol[cEq].DirBC, 1.0, x , psol[cEq].b));

    PetscCall(VecDestroy(&x)); 
    PetscFunctionReturn(0);
}

/*
    Set up PCFIELDSPLIT.
*/
PetscErrorCode petsc_set_pcfieldsplit(const PetscInt dof, const PetscInt cEq)
{   
    IS        isu, isp;
    PetscInt  i, j, ii, jj;
    PetscInt *uindx, *pindx;
    PC pc;

    PetscFunctionBeginUser;

    /* Create index set for velocity and pressure block */
    PetscCall(PetscMalloc2((dof-1)*plhs.mynNo, &uindx, plhs.mynNo, &pindx));
    for (i = 0; i < plhs.mynNo; i++){
        ii = (dof-1)*i;
        jj = dof*plhs.ltg[i];
        for (j = 0; j < dof-1; j++){
            uindx[ii+j] = jj + j;
        }
        pindx[i] = jj + dof - 1;
    }
    PetscCall(ISCreateGeneral(MPI_COMM_WORLD, (dof-1)*plhs.mynNo, uindx, PETSC_COPY_VALUES, &isu));
    PetscCall(ISSetBlockSize(isu, dof-1));
    PetscCall(ISSort(isu));
    PetscCall(ISCreateGeneral(MPI_COMM_WORLD, plhs.mynNo, pindx, PETSC_COPY_VALUES, &isp));
    PetscCall(ISSort(isp));

    /* Pass split information to PETSc. Velocity: block 0, pressure: block 1. */
    PetscCall(KSPGetPC(psol[cEq].ksp, &pc));
    PetscCall(PCFieldSplitSetBlockSize(pc, 2));
    PetscCall(PCFieldSplitSetIS(pc, "0", isu));
    PetscCall(PCFieldSplitSetIS(pc, "1", isp));

    PetscCall(ISDestroy(&isu));
    PetscCall(ISDestroy(&isp));
    PetscCall(PetscFree2(uindx, pindx));
    PetscFunctionReturn(0);
}

/*
    Row-and-Column-Scaling preconditioner.
    \hat{A} = Dr*A*Dc
    \hat{b} = Dr*b
*/
PetscErrorCode petsc_pc_rcs(const PetscInt dof, const PetscInt cEq)
{   
    Vec        Dr, Dc;
    PetscInt   numCol, *row;
    PetscReal *colNorm, *value;
    PetscReal  tol, err1, err2;
    PetscInt   i, iter, maxiter;

    tol     = 0.1;
    maxiter = 10;


    PetscFunctionBeginUser;

    PetscCall(VecSet(psol[cEq].Dr, 1.0));
    PetscCall(VecSet(psol[cEq].Dc, 1.0));
    PetscCall(VecDuplicate(psol[cEq].Dr, &Dr));
    PetscCall(VecDuplicate(psol[cEq].Dc, &Dc));


    for (iter = 0; iter < maxiter; iter++){
        /* Get infinity norm of each row */
        PetscCall(MatGetRowMaxAbs(psol[cEq].A, Dr, NULL));

        /* Get infinity norm of each column */
        PetscCall(MatGetSize(psol[cEq].A, NULL, &numCol));
        PetscCall(PetscMalloc1(numCol, &colNorm));
        PetscCall(MatGetColumnNorms(psol[cEq].A, NORM_INFINITY, colNorm));
        for (i = 0; i < plhs.mynNo; i++) {
            row   = plhs.ltg+i;
            value = colNorm + plhs.ltg[i]*dof;
            PetscCall(VecSetValuesBlocked(Dc, 1, row, value, INSERT_VALUES));
        }
        PetscCall(VecAssemblyBegin(Dc));
        PetscCall(VecAssemblyEnd(Dc));

        /* Calculate 1/sqrt(Dr, Dc) */
        PetscCall(VecSqrtAbs(Dc));
        PetscCall(VecReciprocal(Dc));
        PetscCall(VecSqrtAbs(Dr));
        PetscCall(VecReciprocal(Dr));

        /* Scale A matrix */
        PetscCall(MatDiagonalScale(psol[cEq].A, Dr, Dc));
        PetscCall(VecPointwiseMult(psol[cEq].Dc, psol[cEq].Dc, Dc));
        PetscCall(VecPointwiseMult(psol[cEq].Dr, psol[cEq].Dr, Dr));

        /* Converged or not */
        PetscCall(VecShift(Dc, -1.0));
        PetscCall(VecShift(Dr, -1.0));
        PetscCall(VecNorm(Dc, NORM_INFINITY, &err1));
        PetscCall(VecNorm(Dr, NORM_INFINITY, &err2));
        if (err1 <= tol && err2 <= tol) break;
    }

    if (err1 > tol || err2 > tol ) {
        PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_PC_RCS>: "
                "did not converge to %g in %d iterations.\n"
                "ERROR <PETSC_PC_RCS>: "
                "err1 = %g, err2 = %g\n", tol, maxiter, err1, err2);
    }

    /* Scale right hand side */
    PetscCall(VecPointwiseMult(psol[cEq].b, psol[cEq].b, psol[cEq].Dr));

    PetscCall(PetscFree(colNorm));
    PetscCall(VecDestroy(&Dr));
    PetscCall(VecDestroy(&Dc));

    PetscFunctionReturn(0);
}


/* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

     Private functions for debugging

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */

/*
    Debug: save vector to text file
*/
PetscErrorCode petsc_debug_save_vec(const char *filename, Vec vec)
{   
    PetscFunctionBeginUser;

    PetscViewer viewer;
    PetscViewerASCIIOpen(MPI_COMM_WORLD, filename, &viewer);
    PetscViewerPushFormat(viewer, PETSC_VIEWER_ASCII_COMMON);
    VecView(vec, viewer);
    PetscViewerPopFormat(viewer);
    PetscViewerDestroy(&viewer);

    PetscFunctionReturn(0);
}

/*
    Debug: save matrix to text file
*/
PetscErrorCode petsc_debug_save_mat(const char *filename, Mat mat)
{   
    PetscFunctionBeginUser;

    PetscViewer viewer;
    PetscViewerASCIIOpen(MPI_COMM_WORLD, filename, &viewer);
    PetscViewerPushFormat(viewer, PETSC_VIEWER_ASCII_DENSE);
    MatView(mat, viewer);
    PetscViewerPopFormat(viewer);
    PetscViewerDestroy(&viewer);

    PetscFunctionReturn(0);
}

// Function removing spaces from string (Not used in svFSIplus)
char * rm_blank(char *string)
{
    // remove weird FORTRAN character
    string[strlen(string)-1] = '\0';

    // non_space_count to keep the frequency of non space characters
    int non_space_count = 0;

    //Traverse a string and if it is non space character then, place it at index non_space_count
    for (int i = 0; string[i] != '\0'; i++)
    {
        if (string[i] != ' ')
        {
            string[non_space_count] = string[i];
            non_space_count++;//non_space_count incremented
        }
    }

    //Finally placing final character at the string end
    string[non_space_count] = '\0';
    return string;
}
